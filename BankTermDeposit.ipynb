{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "h1, h2, h3 {\n",
    "    color: black;\n",
    "    font-weight: bold;\n",
    "}\n",
    "h4,h5 {\n",
    "    color: gray\n",
    "}\n",
    "</style>\n",
    "## <span style=\"font-weight:bold;\">Logistic Regression, Decision Tree, KNearest Neighbors, and Support Vector Machines Classifiers </span>\n",
    "### <span style=\"font-weight:bold;\">Overview</span>\n",
    "This project compares the performance of classic classifiers: Logistic Regression, Decision Tree, KNearest Neighbors, and Support Vector Machines\n",
    "using dataset provided by a Portuguese banking institution.  A configuration .ini file is used to selectively control the regressor used allowing \n",
    "individual debugging of each classifier model.\n",
    "Two generic classification functions are defined, the first performing the basic model classification based on the regressor passed in.  The second \n",
    "function utilizes the grid hyper-parameters passed into to perform a GridSearch using the regressor.  Classification model training times, and \n",
    "the model's classification report is returned for comparasions at the end.\n",
    "\n",
    "Classifier performance is performed by comparing Accuracy, Precision, Recall, F1-scoring, model training time from the model classification process.\n",
    "\n",
    "### <span style=\"font-weight:bold;\">Source:</span>  <span style=\"color:black;\">https://archive.ics.uci.edu/dataset/222/bank+marketing</span>\n",
    "The data is a \"Multivariate\" business use data from direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based \n",
    "on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be \n",
    "('yes') or not ('no') subscribed. This project uses the dataset: bank-addition-full.csv with 41,188 rows with 20 columns, ordered by date (from May 2008 to \n",
    "November 2010).  The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n",
    "\n",
    "### <span style=\"font-weight:bold;\">Project Organization</span>\n",
    "The project is organized with the objective that it to be used in an automated environment.  Individual directories, configuration files, and trained models\n",
    "can be wrote out and read back for testing new data.\n",
    "#### <span style=\"font-weight:bold;\">Dataset: </span>./data/bank-additional-full.csv\n",
    "#### <span style=\"font-weight:bold;\">Configuration:</span> ./BankTermDeposit.ini\n",
    "The configuration file serves multiple purposes: it identifies the source of the model training data, controls the train/test data split ratio, manages verbosity, \n",
    "and oversees the activation training and testing of the classifier. Additionally, it specifies the name of the trained model for local storage.\n",
    "#### <span style=\"font-weight:bold;\">Trained Models: </span> specified by model_outputFile in .ini\n",
    "Generated for each of the classifier activated in the configuration file.  Example:  <model_outputFile>_LogisticRegression<timestamp>.pkl.\n",
    "These trained model files are stored in the local directory.  These models can be read back and used for testing to classify new unknown datasets with\n",
    "the same data frame format.\n",
    "#### <span style=\"font-weight:bold;\">Results</span>\n",
    "Classification results from the selected classifiers are tabulated and printed at the end of the process run.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"font-weight:bold;\">Process Flow</span>\n",
    "#### <span style=\"font-weight:bold;\">Configuration</span>\n",
    "Read in necessary system and process flow control related configuration from the supplied .ini file.\n",
    "#### <span style=\"font-weight:bold;\">Data Pre-processing, cleaning</span>\n",
    "Prepare the data by removing bad data, null values, and make the data frame available as the common dataset for later process stages.  Details are provided \n",
    "later in this project.\n",
    "#### <span style=\"font-weight:bold;\">Split data for training and testing</span>\n",
    "The dataset is split according to the to proportions specified in the configuration file: BankTermDeposit.ini, variable: train_test_split, currently set to 0.3 split.\n",
    "\n",
    "#### <span style=\"font-weight:bold;\">Generic Classification Modules</span>\n",
    "These are the 2 generic classifiers functions performing classification, and grid search based on the regressor passed in.  The resulting classification report and \n",
    "measured training time along with accuracy scores are gathered for later comparisions with other regressors.  Classifier training time, or the data fitting time is \n",
    "calculated by timestamping the start and end of the classifier training data fit time.\n",
    "\n",
    "#### <span style=\"font-weight:bold;\">Classification</span>\n",
    "The execution of each of the regressors: Logistic Regression, Decision Tree, KNearest Neighbors, and Support Vector Machines are controlled by ini file's \n",
    "parameters: LogisticRegression, SVMGridSearch,DecisionTreeClassifier,KNNearestNeighbors.  Each module performs basic regressor model classification, and then\n",
    "prepares the proper hyper-parameters for performing grid search.  The results from these classification tasks are gathered, and tabulated later for comparing\n",
    "the different classifier.\n",
    "#### <span style=\"font-weight:bold;\">Results Tabulation</span>\n",
    "This step tabulates the results gathered from all the classification performed.  Based on the size of the dataset, the specific trained module can be stored,\n",
    "and read back directly from a file to be used directly for prediction purposes.\n",
    "\n",
    "## <span style=\"font-weight:bold;\">Classifier Comparison Result</span>\n",
    "\n",
    "Basic Classifiers\n",
    "\n",
    "|    Classifier Model    | Accuracy | Precision | Recall | F1 Score | Train Score | Test Score | Model Fit Time (s) |\n",
    "|------------------------|----------|-----------|--------|----------|-------------|------------|--------------------|\n",
    "|    LinearRegression    |  0.9131  |  0.9017   | 0.9131 |  0.9028  |   0.9082    |   0.9131   |     0.0550551      |\n",
    "|  KNeighborsClassifier  |  0.9037  |  0.8926   | 0.9037 |  0.8962  |   0.9267    |   0.9037   |     0.0156393      |\n",
    "|     SVMGridSearch      |  0.9134  |  0.9017   | 0.9134 |  0.9020  |   0.9185    |   0.9134   |     4.2046297      |\n",
    "| DecisionTreeClassifier |  0.8889  |  0.8930   | 0.8889 |  0.8909  |   1.0000    |   0.8889   |     0.1003885      |\n",
    "\n",
    "\n",
    "GridSearch Best Estimators\n",
    "\n",
    "|    Classifier Model    | Accuracy | Precision | Recall | F1 Score | Train Score | Test Score | Model Fit Time (s) |                         hyper-parameters                         |\n",
    "|------------------------|----------|-----------|--------|----------|-------------|------------|--------------------|------------------------------------------------------------------|\n",
    "|    LinearRegression    |  0.9131  |  0.9017   | 0.9131 |  0.9028  |   0.9081    |   0.9131   |     6.2736654      |      {'regressor__C': 1, 'regressor__solver': 'liblinear'}       |\n",
    "|  KNeighborsClassifier  |  0.9075  |  0.8934   | 0.9075 |  0.8948  |   1.0000    |   0.9075   |     52.1640265     | {'regressor__n_neighbors': 20, 'regressor__weights': 'distance'} |\n",
    "|     SVMGridSearch      |  0.9118  |  0.8994   | 0.9118 |  0.8983  |   0.9130    |   0.9118   |    1197.6210067    |          {'regressor__C': 10, 'regressor__gamma': 0.01}          |\n",
    "| DecisionTreeClassifier |  0.9127  |  0.9094   | 0.9127 |  0.9109  |   0.9345    |   0.9127   |     7.6676013      | {'regressor__max_depth': 10, 'regressor__min_samples_split': 10} |\n",
    "\n",
    "##### Given the same dataset, all classifiers showed similar accuracy and precision perform, with similar training and testing score.\n",
    "##### For this set of banking data, the Logistic, and SVM classifier showing relatively close results showing the data classification favors linear type classification\n",
    "##### Classifiers SVM, and KNearest Neighbors favoring non-linear classification have bad timing performance yielding similar accuracy performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "```\n",
    "Configuration file: BankTermDeposit.ini supplies the necessary configuration to control the process flow of the entire code.\n",
    "Even though this project is presented in a Jupyter Notebook format, however, it can be converted to a python script, and\n",
    "controlled by the configuration file.\n",
    "\n",
    "examples:\n",
    "\n",
    "train_test_split = 0.3\n",
    "model_prefix     = BankTermDesposit_\n",
    "model_outputFile = BankDepositModel_\n",
    "model_inputFile  = BankDepositModel_\n",
    "\n",
    "\n",
    "[DATASET]\n",
    "DataFile = ./data/bank-additional-full.csv\n",
    "....\n",
    "[MODELS]\n",
    "...\n",
    "LogisticRegression     = TRUE\n",
    "SVMGridSearch          = TRUE\n",
    "DecisionTreeClassifier = FALSE\n",
    "KNNearestNeighbors     = TRUE\n",
    "...\n",
    "In this example. DataFile specify the data file path, and its train_test_split portion is specified as 0.3.  The model is prefixed\n",
    "with the name specified, and the trained model to be output and read in from the model_outputFile, and model_inputFile parameters.```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Sections: ['DATASET', 'FEATURE_PROCESSING', 'PROCESS', 'MODELS']\n",
      "LogisticRegression     : TRUE\n",
      "KNNearestNeighbors     : TRUE\n",
      "SVMGridSearch          : TRUE\n",
      "DecisionTreeClassifier : TRUE\n",
      "Model Output File: BankDepositModel_1724741541.9205184.pkl\n",
      "grid_search_verbose: 3\n",
      "DataFile: ./data/bank-additional-full.csv\n",
      "dataset_split: 0.3\n",
      "readback_test: FALSE\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "current_time = str(time.time())\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('BankTermDeposit.ini')\n",
    "print(f'Configuration Sections: {config.sections()}')\n",
    "\n",
    "Proc_LogisticRegression     = config['MODELS']['LogisticRegression'    ]\n",
    "Proc_SVMGridSearch          = config['MODELS']['SVMGridSearch'         ]\n",
    "Proc_KNNearestNeighbors     = config['MODELS']['KNNearestNeighbors'    ]\n",
    "Proc_DecisionTreeClassifier = config['MODELS']['DecisionTreeClassifier']\n",
    "\n",
    "print(f'LogisticRegression     : {Proc_LogisticRegression    }')\n",
    "print(f'KNNearestNeighbors     : {Proc_KNNearestNeighbors    }')\n",
    "print(f'SVMGridSearch          : {Proc_SVMGridSearch         }')\n",
    "print(f'DecisionTreeClassifier : {Proc_DecisionTreeClassifier}')\n",
    "\n",
    "model_outFile = config['DEFAULT']['model_outputFile'] + current_time + '.pkl'\n",
    "model_inFile  = config['DEFAULT']['model_inputFile' ] + current_time + '.pkl'\n",
    "model_prefix_ = config['DEFAULT']['model_prefix'    ]\n",
    "print(f'Model Output File: {model_outFile}')\n",
    "\n",
    "grid_search_verbose = int(config['PROCESS']['gridSearchVerbose'])\n",
    "print(f'grid_search_verbose: {grid_search_verbose}')\n",
    "\n",
    "dataset_file  = config['DATASET']['DataFile']\n",
    "print(f'DataFile: {dataset_file}')\n",
    "\n",
    "dataset_split = float(config['DEFAULT']['train_test_split'])\n",
    "print(f'dataset_split: {dataset_split}')\n",
    "\n",
    "readback_test = config['MODELS']['ReadBackTest']\n",
    "print(f'readback_test: {readback_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "### Remove Duplicates\n",
    "None found\n",
    "### Strip extra \"\" characters\n",
    "Discoverd that even though the dataset isin the csv delimited ';' format; additional \"\" between the field data causes panda's pd.read_csv unable to read the field data and\n",
    "populate it into the proper column.\n",
    "### Create DataFrame\n",
    "Create new data frame based on the processed column from prior step.  From dataset documentation, setup a mapping dictionary which maps the column name and the type of the column.\n",
    "Three types of column types found:  numeric (integer), categorial, and binary (output class). \n",
    "Next perform the same character replacement procedure for all the data in the dataset and populate the entire data frame.\n",
    "### Map / Convert 'month', 'day_of_week' columns\n",
    "Map and convert to numeric data for these columns\n",
    "### Label Encoding Categorical and Binary Columns\n",
    "For the remaining categorical, and binary columns, perform LabelEncoder to convert data to numeric numbers\n",
    "### Training, Testing data split\n",
    "Split the dataset and prepare for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_prefix = model_prefix_\n",
    "df_dataset = dataset_file\n",
    "\n",
    "df_split   = dataset_split\n",
    "\n",
    "df = pd.read_csv( df_dataset )\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "##  Dataset Pre-Processing                                                            ##\n",
    "########################################################################################\n",
    "dataset_dtypes = {\n",
    "    'age'            : 'integer',\n",
    "    'job'            : 'categorical',\n",
    "    'marital'        : 'categorical',\n",
    "    'education'      : 'categorical',\n",
    "    'default'        : 'categorical',\n",
    "    'housing'        : 'categorical',\n",
    "    'loan'           : 'categorical',\n",
    "    'contact'        : 'categorical',\n",
    "    'month'          : 'categorical',\n",
    "    'day_of_week'    : 'categorical',\n",
    "    'duration'       : 'integer',\n",
    "    'campaign'       : 'integer',\n",
    "    'pdays'          : 'integer',\n",
    "    'previous'       : 'integer',\n",
    "    'poutcome'       : 'categorical',\n",
    "    'emp.var.rate'   : 'integer',\n",
    "    'cons.price.idx' : 'integer',\n",
    "    'cons.conf.idx'  : 'integer',\n",
    "    'euribor3m'      : 'integer',\n",
    "    'nr.employed'    : 'integer',\n",
    "    'y'              : 'binary',\n",
    "}\n",
    "\n",
    "# remove extra \"\" from column name, and field data\n",
    "data_column = df.columns\n",
    "col_names = data_column.str.split(';')\n",
    "col_names = [[elem.replace('\"', '') for elem in sublist] for sublist in col_names] \n",
    "\n",
    "col_names = [item for sublist in col_names for item in sublist]     # flatten column names\n",
    "\n",
    "orig_column = df.columns[0]\n",
    "#print(f'{type(orig_column), {orig_column}}')\n",
    "# Split the data in the original column into multiple columns\n",
    "df[orig_column] = df[orig_column].astype(str)\n",
    "\n",
    "split_data = df[orig_column].str.split(';', expand=True)\n",
    "\n",
    "# remove extra \"\" for entire data\n",
    "split_data = split_data.applymap(lambda x: x.replace('\"', '') if x else x)\n",
    "\n",
    "# Check the number of columns in split_data\n",
    "num_cols = split_data.shape[1]\n",
    "\n",
    "# Generate column names dynamically or trim the existing col_names list\n",
    "if len(col_names) == num_cols:\n",
    "    split_data.columns = col_names\n",
    "else:\n",
    "    split_data.columns = [f'{orig_column}_{i}' for i in range(num_cols)]\n",
    "\n",
    "df = split_data.copy()\n",
    "\n",
    "# map column to documented data type\n",
    "for col in df.columns:\n",
    "    dftype = dataset_dtypes[col]\n",
    "    if dftype == 'integer':\n",
    "        df[col] = pd.to_numeric( df[col],errors='coerce').astype('int64')\n",
    "    elif dftype == 'float':\n",
    "        df[col] = pd.to_numeric( df[col],errors='coerce').astype('float64')\n",
    "    else:\n",
    "        df[col] = df[col].astype('category')\n",
    "#    print(f'[{col}] ; {df[col].dtype}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Month and Weekday Features\n",
    "month_mapping = {\n",
    "    'jan'  : 1, 'feb' : 2,  'mar'  : 3,  'apr' : 4,\n",
    "    'may'  : 5, 'jun' : 6,  'jul'  : 7,  'aug' : 8,\n",
    "    'sep'  : 9, 'oct' : 10, 'nov'  : 11, 'dec' : 12\n",
    "}\n",
    "\n",
    "weekday_mapping = {\n",
    "    'mon': 1, 'tue' : 2, 'wed' : 3, 'thu': 4,\n",
    "    'fri': 5, 'sat' : 6, 'sun' : 7\n",
    "}\n",
    "\n",
    "# LabelEncode the rest of the columns to numeric values\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['category']).columns:\n",
    "    if column not in ['month', 'day_of_week']:\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "        \n",
    "df['month']       = df['month'].map(month_mapping)\n",
    "df['day_of_week'] = df['day_of_week'].map(weekday_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Missing Values Count per Column:\n",
      "age               0\n",
      "job               0\n",
      "marital           0\n",
      "education         0\n",
      "default           0\n",
      "housing           0\n",
      "loan              0\n",
      "contact           0\n",
      "month             0\n",
      "day_of_week       0\n",
      "duration          0\n",
      "campaign          0\n",
      "pdays             0\n",
      "previous          0\n",
      "poutcome          0\n",
      "emp.var.rate      0\n",
      "cons.price.idx    0\n",
      "cons.conf.idx     0\n",
      "euribor3m         0\n",
      "nr.employed       0\n",
      "y                 0\n",
      "dtype: int64\n",
      "Number of duplicate rows: 1\n",
      "Number of rows after removing duplicates: 41175\n"
     ]
    }
   ],
   "source": [
    "## print(f'{df.head(10)}')\n",
    "# Display the number of missing values in each column\n",
    "print(\"\\n Missing Values Count per Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Number of rows after removing duplicates: {df.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  job  marital  education  default  housing  loan  contact month  \\\n",
      "0   56    3        1          0        0        0     0        1     5   \n",
      "1   57    7        1          3        1        0     0        1     5   \n",
      "2   37    7        1          3        0        2     0        1     5   \n",
      "3   40    0        1          1        0        0     0        1     5   \n",
      "4   56    7        1          3        0        0     2        1     5   \n",
      "\n",
      "  day_of_week  duration  campaign  pdays  previous  poutcome  emp.var.rate  \\\n",
      "0           1       261         1    999         0         1             1   \n",
      "1           1       149         1    999         0         1             1   \n",
      "2           1       226         1    999         0         1             1   \n",
      "3           1       151         1    999         0         1             1   \n",
      "4           1       307         1    999         0         1             1   \n",
      "\n",
      "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed  \n",
      "0              93            -36          4         5191  \n",
      "1              93            -36          4         5191  \n",
      "2              93            -36          4         5191  \n",
      "3              93            -36          4         5191  \n",
      "4              93            -36          4         5191  \n",
      "[age] : int64\n",
      "[job] : int32\n",
      "[marital] : int32\n",
      "[education] : int32\n",
      "[default] : int32\n",
      "[housing] : int32\n",
      "[loan] : int32\n",
      "[contact] : int32\n",
      "[month] : category\n",
      "[day_of_week] : category\n",
      "[duration] : int64\n",
      "[campaign] : int64\n",
      "[pdays] : int64\n",
      "[previous] : int64\n",
      "[poutcome] : int32\n",
      "[emp.var.rate] : int64\n",
      "[cons.price.idx] : int64\n",
      "[cons.conf.idx] : int64\n",
      "[euribor3m] : int64\n",
      "[nr.employed] : int64\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "##  Dataset Preprocessing                                         ##\n",
    "##  checking                                                      ##\n",
    "####################################################################\n",
    "\n",
    "x = df.drop(columns='y',axis=1)\n",
    "y  = df['y']\n",
    "print(f'{x.head()}')\n",
    "for col in x.columns:\n",
    "    print(f'[{col}] : {df[col].dtype}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for training and testing\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=df_split, random_state=12)\n",
    "\n",
    "models_data = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "##  Perform training data fitting, and the compute classification ##\n",
    "##  result.  Keep track of model training time                    ##   \n",
    "####################################################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def ModelClassification(regressor, regressor_text, X_train,y_train, X_test,y_test):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model_regressor = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    model_fit_param = model_regressor.fit(X_train, y_train)\n",
    "\n",
    "    model_train_time = time.time() - start_time\n",
    "\n",
    "    y_pred                 = model_regressor.predict (X_test)\n",
    "    model_accuracy         = accuracy_score(y_test,y_pred)\n",
    "    model_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    report_classification  = classification_report(y_test,y_pred)\n",
    "    report_classification_ = classification_report(y_test,y_pred,output_dict=True)\n",
    "\n",
    "    model_f1_score         = f1_score( y_test,y_pred,average='weighted')\n",
    "\n",
    "    training_score = model_regressor.score(X_train,y_train)\n",
    "    testing_score  = model_regressor.score(X_test,y_test)\n",
    "\n",
    "    classifier_report = {\n",
    "        'train_time'       : [ model_train_time ],\n",
    "        'fit_parameters'   : [ model_fit_param ],\n",
    "        'model_regressor'  : [ model_regressor ],\n",
    "        'model_accuracy'   : [ model_accuracy ],\n",
    "        'model_precision'  : [report_classification_['weighted avg']['precision']],\n",
    "        'model_recall'     : [report_classification_['weighted avg']['recall']],\n",
    "        'model_f1_score'   : [report_classification_['weighted avg']['f1-score']],\n",
    "        'confusion_matrix' : [ model_confusion_matrix ],\n",
    "        'classify_report'  : [ report_classification, report_classification_ ],\n",
    "        'train_score'      : [ training_score ],\n",
    "        'test_score'       : [ testing_score ]\n",
    "    }\n",
    "##    print(f'regressor: {regressor_text}\\n{classifier_report}')\n",
    "    return classifier_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "##  Perform training data fitting, and the compute classification ##\n",
    "##  result.  Keep track of model training time/ This is the Grid  ##\n",
    "##  Search version                                                ##   \n",
    "####################################################################\n",
    "\n",
    "def ModelClassification_GridSearch(regressor, regressor_text, X_train,y_train,X_test,y_test,param_grid):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model_regressor = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "    grid_search = GridSearchCV(model_regressor, param_grid, cv=10, scoring='accuracy')\n",
    "    model_fit_param = grid_search.fit(X_train, y_train)\n",
    "    best_regressor  = grid_search.best_estimator_\n",
    "\n",
    "    model_train_time = time.time() - start_time\n",
    "\n",
    "    y_pred                 = best_regressor.predict(X_test)\n",
    "    model_accuracy         = accuracy_score(y_test,y_pred)\n",
    "    model_confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    \n",
    "    report_classification = classification_report(y_test,y_pred)\n",
    "    report_classification_= classification_report(y_test,y_pred, output_dict=True)\n",
    "    model_f1_score        = f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "    grid_training_score = best_regressor.score(X_train,y_train)\n",
    "    grid_testing_score  = best_regressor.score(X_test, y_test )\n",
    "\n",
    "    classifier_report = {\n",
    "        'train_time'       : [ model_train_time ],\n",
    "        'model_regressor'  : [ grid_search ],\n",
    "        'model_accuracy'   : [ model_accuracy ],\n",
    "        'model_precision'  : [report_classification_['weighted avg']['precision']],\n",
    "        'model_recall'     : [report_classification_['weighted avg']['recall']],\n",
    "        'model_f1_score'   : [report_classification_['weighted avg']['f1-score']],\n",
    "        'fit_parameters'   : [ model_fit_param ],\n",
    "        'confusion_matrix' : [ model_confusion_matrix ],\n",
    "        'classify_report'  : [ report_classification,report_classification_ ],\n",
    "        'train_score'      : [ grid_training_score ],\n",
    "        'test_score'       : [ grid_testing_score ]\n",
    "    }\n",
    "##    print(f'regressor: {regressor_text}\\n{classifier_report}')\n",
    "    return classifier_report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression model save to: BankTermDesposit_LinearRegression1724741541.9205184.pkl\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "## Perform the basic Logistic Regression, and its GridSearchCV    ##\n",
    "## version with the supplied search hyper-parameter ranges.       ##\n",
    "## Process controlled by configuration file.                      ##\n",
    "####################################################################\n",
    "if Proc_LogisticRegression == 'TRUE':\n",
    "    classifier_report = ModelClassification(LogisticRegression(), 'Logistic Regression', X_train, y_train, X_test, y_test) \n",
    "    param_grid_lr = {\n",
    "        'regressor__C': [0.1, 1, 10],\n",
    "        'regressor__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    \n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(LogisticRegression(), 'Logistic Regression GridSearch', X_train, y_train, X_test, y_test, param_grid_lr)\n",
    "    models_data['LinearRegression'] = [ classifier_report, classifier_gridsearch_report ]\n",
    "\n",
    "    model_outputFile_ = model_prefix_ + 'LinearRegression' + current_time +'.pkl'\n",
    "    with open( model_outputFile_,'wb') as model_file:\n",
    "        pickle.dump(models_data['LinearRegression'],model_file)\n",
    "        print(f'LinearRegression model save to: {model_outputFile_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier model save to: BankTermDesposit_KNeighborsClassifier1724741541.9205184.pkl\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "## Perform the basic KNNeighbors regression, and its GridSearchCV ##\n",
    "## version with the supplied search hyper-parameter ranges.       ##\n",
    "## Process controlled by configuration file.                      ##\n",
    "####################################################################\n",
    "\n",
    "if Proc_KNNearestNeighbors == 'TRUE':\n",
    "    classifier_report = ModelClassification(KNeighborsClassifier(), 'KNeighborsClassifier', X_train, y_train, X_test, y_test) \n",
    "    \n",
    "    param_grid_knn = {\n",
    "        'regressor__n_neighbors': np.arange(1, 21),\n",
    "        'regressor__weights': ['uniform', 'distance']\n",
    "    }\n",
    "    \n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(KNeighborsClassifier(), 'KNeighborsClassifier', X_train, y_train, X_test, y_test, param_grid_knn) \n",
    "    models_data['KNeighborsClassifier'] = [ classifier_report, classifier_gridsearch_report ]\n",
    "\n",
    "    model_outputFile_ = model_prefix_ + 'KNeighborsClassifier' + current_time +'.pkl'\n",
    "    with open( model_outputFile_,'wb') as model_file:\n",
    "        pickle.dump(models_data['KNeighborsClassifier'],model_file)\n",
    "        print(f'KNeighborsClassifier model save to: {model_outputFile_}')\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMGridSearch model save to: BankTermDesposit_SVMGridSearch1724741541.9205184.pkl\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "## Perform the basic Support Vector Machines,and its GridSearchCV ##\n",
    "## version with the supplied search hyper-parameter ranges.       ##\n",
    "## Process controlled by configuration file.                      ##\n",
    "####################################################################\n",
    "\n",
    "if Proc_SVMGridSearch == 'TRUE':\n",
    "    classifier_report = ModelClassification(SVC(), 'SVMGridSearch', X_train, y_train, X_test, y_test) \n",
    "    param_grid_svc = {\n",
    "        'regressor__C': [0.1,1, 10], \n",
    "        'regressor__gamma': [1,0.1,0.01]\n",
    "    } \n",
    "    \n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(SVC(), 'SVMGridSearch', X_train, y_train, X_test, y_test, param_grid_svc) \n",
    "    models_data['SVMGridSearch'] = [ classifier_report, classifier_gridsearch_report ]\n",
    "\n",
    "    model_outputFile_ = model_prefix_ + 'SVMGridSearch' + current_time +'.pkl'\n",
    "    with open( model_outputFile_,'wb') as model_file:\n",
    "        pickle.dump(models_data['SVMGridSearch'],model_file)\n",
    "        print(f'SVMGridSearch model save to: {model_outputFile_}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier model save to: BankTermDesposit_DecisionTreeClassifier1724741541.9205184.pkl\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## Perform the basic DecisionTree Classification, and its GridSearchCV ##\n",
    "## version with the supplied search hyper-parameter ranges.            ##\n",
    "## Process controlled by configuration file.                           ##\n",
    "#########################################################################\n",
    "\n",
    "if Proc_DecisionTreeClassifier == 'TRUE':\n",
    "    classifier_report = ModelClassification(DecisionTreeClassifier(), 'DecisionTreeClassifier', X_train, y_train, X_test, y_test) \n",
    "    param_grid_dtc = {\n",
    "        'regressor__max_depth': [None, 10, 20],\n",
    "        'regressor__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(DecisionTreeClassifier(), 'DecisionTreeClassifier', X_train, y_train, X_test, y_test, param_grid_dtc) \n",
    "    models_data['DecisionTreeClassifier'] = [ classifier_report, classifier_gridsearch_report ]\n",
    "\n",
    "    model_outputFile_ = model_prefix_ + 'DecisionTreeClassifier' + current_time +'.pkl'\n",
    "    with open( model_outputFile_,'wb') as model_file:\n",
    "        pickle.dump(models_data['DecisionTreeClassifier'],model_file)\n",
    "        print(f'DecisionTreeClassifier model save to: {model_outputFile_}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Basic Classifiers\n",
      "+------------------------+----------+-----------+--------+----------+-------------+------------+--------------------+\n",
      "|    Classifier Model    | Accuracy | Precision | Recall | F1 Score | Train Score | Test Score | Model Fit Time (s) |\n",
      "+------------------------+----------+-----------+--------+----------+-------------+------------+--------------------+\n",
      "|    LinearRegression    |  0.9131  |  0.9017   | 0.9131 |  0.9028  |   0.9082    |   0.9131   |     0.0597091      |\n",
      "|  KNeighborsClassifier  |  0.9037  |  0.8926   | 0.9037 |  0.8962  |   0.9267    |   0.9037   |     0.0281515      |\n",
      "|     SVMGridSearch      |  0.9134  |  0.9017   | 0.9134 |  0.9020  |   0.9185    |   0.9134   |     3.9079716      |\n",
      "| DecisionTreeClassifier |  0.8877  |  0.8921   | 0.8877 |  0.8898  |   1.0000    |   0.8877   |     0.1100681      |\n",
      "+------------------------+----------+-----------+--------+----------+-------------+------------+--------------------+\n",
      "\n",
      "GridSearch Best Estimators\n",
      "+------------------------+----------+-----------+--------+----------+-------------+------------+--------------------+------------------------------------------------------------------+\n",
      "|    Classifier Model    | Accuracy | Precision | Recall | F1 Score | Train Score | Test Score | Model Fit Time (s) |                         hyper-parameters                         |\n",
      "+------------------------+----------+-----------+--------+----------+-------------+------------+--------------------+------------------------------------------------------------------+\n",
      "|    LinearRegression    |  0.9131  |  0.9017   | 0.9131 |  0.9028  |   0.9081    |   0.9131   |     6.7026401      |      {'regressor__C': 1, 'regressor__solver': 'liblinear'}       |\n",
      "|  KNeighborsClassifier  |  0.9075  |  0.8934   | 0.9075 |  0.8948  |   1.0000    |   0.9075   |     55.0292523     | {'regressor__n_neighbors': 20, 'regressor__weights': 'distance'} |\n",
      "|     SVMGridSearch      |  0.9118  |  0.8994   | 0.9118 |  0.8983  |   0.9130    |   0.9118   |    1200.4652083    |          {'regressor__C': 10, 'regressor__gamma': 0.01}          |\n",
      "| DecisionTreeClassifier |  0.9131  |  0.9099   | 0.9131 |  0.9113  |   0.9371    |   0.9131   |     7.8972330      | {'regressor__max_depth': 10, 'regressor__min_samples_split': 5}  |\n",
      "+------------------------+----------+-----------+--------+----------+-------------+------------+--------------------+------------------------------------------------------------------+\n",
      "\n",
      "saved models to BankDepositModel_1724741541.9205184.pkl\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "## Tabulate all the results from the classification models.  Output    ##\n",
    "## model to file for another system to go directly to prediction       ##\n",
    "## without training the model.                                         ##\n",
    "#########################################################################\n",
    "from tabulate import tabulate\n",
    "\n",
    "f_row, f_acc, f_prec, f_f1, f_recall = 'weighted avg','model_accuracy','model_precision','model_f1_score','model_recall'\n",
    "f_train, f_trainscore, f_testscore = 'train_time','train_score','test_score'\n",
    "\n",
    "headers = ['Classifier Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Train Score', 'Test Score', 'Model Fit Time (s)']\n",
    "tab_data_basic = []\n",
    "tab_data_grids = []\n",
    "for model, report_ in models_data.items():\n",
    "    c_report      = report_[0]\n",
    "    b_accuracy    = c_report[f_acc][0]\n",
    "    b_precision   = c_report[f_prec][0]\n",
    "    b_recall      = c_report[f_recall][0]\n",
    "    b_f1_score    = c_report[f_f1][0]\n",
    "    b_train_time  = c_report[f_train][0]\n",
    "    b_train_score = c_report[f_trainscore][0]\n",
    "    b_test_score  = c_report[f_testscore][0]\n",
    "    b_model       = c_report['model_regressor'][0]\n",
    "\n",
    "    g_report      = report_[1]\n",
    "    g_accuracy    = g_report[f_acc][0]\n",
    "    g_precision   = g_report[f_prec][0]\n",
    "    g_recall      = g_report[f_recall][0]\n",
    "    g_f1_score    = g_report[f_f1][0]\n",
    "    g_train_time  = g_report[f_train][0]\n",
    "    g_train_score = g_report[f_trainscore][0]\n",
    "    g_test_score  = g_report[f_testscore][0]\n",
    "    gsearch       = g_report['model_regressor'][0]\n",
    "\n",
    "    tdata = [ f'{model}', f'{b_accuracy:.4f}', f'{b_precision:.4f}',  f'{b_recall:.4f}',  f'{b_f1_score:.4f}', f'{b_train_score:.4f}', f'{b_test_score:.4f}', f'{b_train_time:.7f}' ]\n",
    "    tab_data_basic.append( tdata )\n",
    "\n",
    "    tdata_g = [ f'{model}', f'{g_accuracy:.4f}', f'{g_precision:.4f}',  f'{g_recall:.4f}',  f'{g_f1_score:.4f}', f'{g_train_score:.4f}' ,f'{g_test_score:.4f}', f'{g_train_time:.7f}', f'{gsearch.best_params_}' ]\n",
    "    tab_data_grids.append( tdata_g )\n",
    "\n",
    "print()\n",
    "##print(f'*****  DataSet: {df_dataset}, test/train split: {df_split}  *****')\n",
    "print()\n",
    "basic_table = tabulate(tab_data_basic, headers, tablefmt='pretty')\n",
    "title = 'Basic Classifiers'\n",
    "basic_full_table = f'{title}\\n{basic_table}'\n",
    "print(basic_full_table)\n",
    "print()\n",
    "\n",
    "gs_headers = ['Classifier Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Train Score', 'Test Score', 'Model Fit Time (s)', 'hyper-parameters']\n",
    "gs_table = tabulate(tab_data_grids, gs_headers, tablefmt='pretty')\n",
    "title = 'GridSearch Best Estimators'\n",
    "gs_full_table = f'{title}\\n{gs_table}'\n",
    "print(gs_full_table)\n",
    "print()\n",
    "\n",
    "import pickle\n",
    "with open(model_outFile,'wb') as model_file:\n",
    "    pickle.dump(models_data, model_file)\n",
    "    print(f'saved models to {model_outFile}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if readback_test == 'TRUE':\n",
    "    with open(model_outFile,'wb') as model_file:\n",
    "        pickle.dump(models_data, model_file)\n",
    "        print(f'saved models to {model_outFile}')\n",
    "    \n",
    "    # Load file back ###########################################################\n",
    "    with open(model_inFile,'rb') as input_model:\n",
    "        mdata = pickle.load(input_model)\n",
    "        print(f'Loaded model data from: {model_inFile}')\n",
    "    \n",
    "    tab1_data_basic = []\n",
    "    tab1_data_grids = []\n",
    "    model_detectors = defaultdict()\n",
    "    \n",
    "    for model, report_ in mdata.items():\n",
    "        c_report      = report_[0]\n",
    "        b_accuracy    = c_report[f_acc][0]\n",
    "        b_precision   = c_report[f_prec][0]\n",
    "        b_recall      = c_report[f_recall][0]\n",
    "        b_f1_score    = c_report[f_f1][0]\n",
    "        b_train_time  = c_report[f_train][0]\n",
    "        b_train_score = c_report[f_trainscore][0]\n",
    "        b_test_score  = c_report[f_testscore][0]\n",
    "    \n",
    "        g_report      = report_[1]\n",
    "        g_accuracy    = g_report[f_acc][0]\n",
    "        g_precision   = g_report[f_prec][0]\n",
    "        g_recall      = g_report[f_recall][0]\n",
    "        g_f1_score    = g_report[f_f1][0]\n",
    "        g_train_time  = g_report[f_train][0]\n",
    "        g_train_score = g_report[f_trainscore][0]\n",
    "        g_test_score  = g_report[f_testscore][0]\n",
    "        gsearch       = g_report['model_regressor'][0]\n",
    "    \n",
    "        model_detectors[ model ] = gsearch\n",
    "    \n",
    "        tdata = [ f'{model}', f'{b_accuracy:.4f}', f'{b_precision:.4f}',  f'{b_recall:.4f}',  f'{b_f1_score:.4f}', f'{b_train_score:.4f}', f'{b_test_score:.4f}', f'{b_train_time:.7f}' ]\n",
    "        tab1_data_basic.append( tdata )\n",
    "    \n",
    "        tdata_g = [ f'{model}', f'{g_accuracy:.4f}', f'{g_precision:.4f}',  f'{g_recall:.4f}',  f'{g_f1_score:.4f}', f'{g_train_score:.4f}' ,f'{g_test_score:.4f}', f'{g_train_time:.7f}', f'{gsearch.best_params_}' ]\n",
    "        tab1_data_grids.append( tdata_g )\n",
    "    \n",
    "    print()\n",
    "    ##print(f'*****  DataSet: {df_dataset}, test/train split: {df_split}  *****')\n",
    "    print()\n",
    "    basic_table = tabulate(tab1_data_basic, headers, tablefmt='pretty')\n",
    "    title = 'Basic Classifiers'\n",
    "    basic_full_table = f'{title}\\n{basic_table}'\n",
    "    print(basic_full_table)\n",
    "    print()\n",
    "    \n",
    "    gs_headers = ['Classifier Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Train Score', 'Test Score', 'Model Fit Time (s)', 'hyper-parameters']\n",
    "    gs_table = tabulate(tab1_data_grids, gs_headers, tablefmt='pretty')\n",
    "    title = 'GridSearch Best Estimators'\n",
    "    gs_full_table = f'{title}\\n{gs_table}'\n",
    "    print(gs_full_table)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
